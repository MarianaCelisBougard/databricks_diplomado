{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d66839e3-b88b-486e-98f0-1cc2deb1e68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos con requests y leerlos en pandas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000\"\n",
    "url_men = \"https://www.datos.gov.co/resource/nudc-7mev.csv?$limit=100000\"\n",
    " \n",
    "\n",
    "# Descargar contenido\n",
    "response_secop = requests.get(url_secop)\n",
    "response_men = requests.get(url_men)\n",
    "\n",
    "# Convertir contenido a pandas usando StringIO\n",
    "df_secop_pd = pd.read_csv(StringIO(response_secop.text))\n",
    "df_men_pd = pd.read_csv(StringIO(response_men.text))\n",
    "\n",
    "# Convertir pandas a Spark\n",
    "df_secop = spark.createDataFrame(df_secop_pd)\n",
    "df_men = spark.createDataFrame(df_men_pd)\n",
    "\n",
    "# Mostrar en Databricks\n",
    "display(df_secop)\n",
    "display(df_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422bd09d-fe87-4d5b-a573-16b049af1578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 2: Guardar los DataFrames como tablas Delta\n",
    "# La función .saveAsTable() guarda los datos y registra la tabla en el Unity Catalog.\n",
    "# El modo \"overwrite\" reemplaza la tabla si ya existe, ideal para actualizaciones.\n",
    "spark.sql(\"DROP TABLE IF EXISTS main.diplomado.secop\")\n",
    "df_secop.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado.secop\")\n",
    "\n",
    "\n",
    "df_men.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado.men_estadisticas\")\n",
    "\n",
    "print(\"¡Tablas guardadas exitosamente en el catálogo 'main', esquema 'diplomado_datos'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc4eec9-ef46-4e6c-a317-5154349d11b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos con requests y leerlos en pandas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?limit=100000&offset=100000\"\n",
    " \n",
    "# Descargar contenido\n",
    "response_secop = requests.get(url_secop)\n",
    "\n",
    "# Convertir contenido a pandas usando StringIO\n",
    "df_secop_pd = pd.read_csv(StringIO(response_secop.text), error_bad_lines=False, sep=',', dtype=str)\n",
    "\n",
    "# Convertir pandas a Spark\n",
    "df_secop = spark.createDataFrame(df_secop_pd)\n",
    "\n",
    "\n",
    "# Mostrar en Databricks\n",
    "display(df_secop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a9bb1b5-3f01-4e8a-a90c-eed2c0c5a344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "target_schema = spark.table(\"main.diplomado.secop\").schema\n",
    "\n",
    "df_secop_aligned = df_secop.select(\n",
    "    [col(field.name).cast(field.dataType) for field in target_schema.fields]\n",
    ")\n",
    "\n",
    "\n",
    "df_secop_aligned.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"main.diplomado.secop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "878bbb03-c211-474d-9b37-7c0a04f99375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook – Ingesta SECOP II (~19,4 M filas) \n",
    "# ============================================================================\n",
    "# 1️⃣  IMPORTS Y SESIÓN SPARK\n",
    "import requests, time, math, pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ============================================================================\n",
    "# 2️⃣  PARÁMETROS\n",
    "BASE_URL      = \"https://www.datos.gov.co/resource/rpmr-utcd.csv\"\n",
    "CHUNK_SIZE    = 100_000           # filas por bloque\n",
    "TOTAL_ROWS    = 19_400_000        # aprox. – solo para progreso\n",
    "MAX_RETRIES   = 3                 # reintentos de red\n",
    "SLEEP_SEC     = 1                 # pausa entre llamadas\n",
    "TARGET_TABLE  = \"main.diplomado.secop\"\n",
    "START_OFFSET  = 0                 # pon >0 si reanudas\n",
    "# spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")  # alternativa rápida\n",
    "\n",
    "# ============================================================================\n",
    "# 3️⃣  ESQUEMA DEL DESTINO\n",
    "target_schema = spark.table(TARGET_TABLE).schema\n",
    "\n",
    "# ============================================================================\n",
    "# 4️⃣  FUNCIÓN TRY_CAST SEGURA\n",
    "def safe_cast(col_name: str, spark_dtype: str):\n",
    "    return expr(f\"try_cast(`{col_name}` AS {spark_dtype})\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5️⃣  BUCLE PRINCIPAL\n",
    "n_loops = math.ceil(TOTAL_ROWS / CHUNK_SIZE)\n",
    "offset  = START_OFFSET\n",
    "bloc    = (offset // CHUNK_SIZE) + 1\n",
    "\n",
    "while True:\n",
    "    url = f\"{BASE_URL}?$limit={CHUNK_SIZE}&$offset={offset}\"\n",
    "\n",
    "    # ---------- Descarga con reintentos ----------\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=60)\n",
    "            resp.raise_for_status()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt == MAX_RETRIES:\n",
    "                raise RuntimeError(f\"Fallo tras {MAX_RETRIES} intentos en offset {offset}: {e}\")\n",
    "            time.sleep(2 * attempt)\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    pdf = pd.read_csv(StringIO(resp.text), dtype=str)\n",
    "    if pdf.empty:\n",
    "        print(f\"Fin de datos en offset {offset:,}. ¡Carga completa!\")\n",
    "        break\n",
    "\n",
    "    # ---------- Pandas → Spark ----------\n",
    "    sdf_raw = spark.createDataFrame(pdf)\n",
    "\n",
    "    # ---------- Alineación + try_cast ----------\n",
    "    sdf = sdf_raw.select(\n",
    "        [\n",
    "            (\n",
    "                safe_cast(field.name, field.dataType.simpleString())\n",
    "                if field.name in sdf_raw.columns\n",
    "                else expr(\"NULL\").cast(field.dataType)\n",
    "            ).alias(field.name)\n",
    "            for field in target_schema.fields\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---------- Escritura Delta ----------\n",
    "    write_mode = \"overwrite\" if offset == 0 else \"append\"\n",
    "    (sdf.write\n",
    "        .format(\"delta\")\n",
    "        .mode(write_mode)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .saveAsTable(TARGET_TABLE)\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Bloque {bloc}/{n_loops} cargado ({len(pdf):,} filas, offset {offset:,}, modo {write_mode}).\")\n",
    "\n",
    "    # Preparar siguiente iteración\n",
    "    offset += CHUNK_SIZE\n",
    "    bloc   += 1\n",
    "    time.sleep(SLEEP_SEC)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(20250705)Tranformación de Datos.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
